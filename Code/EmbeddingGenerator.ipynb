{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> db804517e34a907e9356fc13015da1921797878d
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> db804517e34a907e9356fc13015da1921797878d
   "source": [
    "# Use word embeddings by Google, directly from the file\n",
    "embeddings = KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> db804517e34a907e9356fc13015da1921797878d
   "source": [
    "# To Get Processed data tokens from the text file\n",
    "def preprocessFile(filename):\n",
    "    f = open(filename,'r')\n",
    "    wordlist = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for line in f:\n",
    "        #Step 1: Removing Punctuations\n",
    "        line=\"\".join([i for i in line if i not in string.punctuation])\n",
    "        #Step 2: Converting to lower case\n",
    "        line = line.lower()\n",
    "        #Step 3: Removing stopwords from the text\n",
    "        line = remove_stopwords(line)\n",
    "        #Step 4: Tokenization\n",
    "        line = line.split()\n",
    "        #Step 5: Lemmetization (Preffered over Stemming)\n",
    "        for word in line:\n",
    "            wordlist.append(wnl.lemmatize(word))\n",
    "    return wordlist\n",
    "\n",
    "# To get a single Video embeddings from the transcript            \n",
    "def getEmbedding(filename,embeddings):\n",
    "    wordlist = preprocessFile(filename)\n",
    "    embedding = np.zeros((300,))\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            embedding += embeddings[word]\n",
    "        except:\n",
    "            print(\"word not in dictionary\")\n",
    "    embedding /= len(wordlist)\n",
    "    return embedding"
<<<<<<< HEAD
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "file = \"../Data/sampledata.txt\"\n",
    "vec = getEmbedding(filename=file,embeddings=embeddings)\n",
    "most_similar = embeddings.most_similar(positive=[vec],topn=3)\n",
    "print(most_similar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('kinda_fishy', 0.5858803391456604), ('confusion_Saljic', 0.572158932685852), ('fun', 0.5702428221702576)]\n"
     ]
    }
   ],
   "metadata": {}
=======
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/nsr/intern/Recommendation-Engine/Data/sampledata.txt'\n",
    "getEmbedding(filename,embeddings)"
   ]
>>>>>>> db804517e34a907e9356fc13015da1921797878d
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}