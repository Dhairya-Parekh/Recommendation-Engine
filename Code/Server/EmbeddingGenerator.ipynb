{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import string"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Use word embeddings by Google, directly from the file\n",
    "embeddings = KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# To Get Processed data tokens from the text file\n",
    "def preprocessFile(filename):\n",
    "    f = open(filename,'r')\n",
    "    wordlist = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for line in f:\n",
    "        #Step 1: Removing Punctuations\n",
    "        line=\"\".join([i for i in line if i not in string.punctuation])\n",
    "        #Step 2: Converting to lower case\n",
    "        line = line.lower()\n",
    "        #Step 3: Removing stopwords from the text\n",
    "        line = remove_stopwords(line)\n",
    "        #Step 4: Tokenization\n",
    "        line = line.split()\n",
    "        #Step 5: Lemmetization (Preffered over Stemming)\n",
    "        for word in line:\n",
    "            wordlist.append(wnl.lemmatize(word))\n",
    "    return wordlist\n",
    "\n",
    "# To get a single Video embeddings from the transcript            \n",
    "def getEmbedding(filename,embeddings):\n",
    "    wordlist = preprocessFile(filename)\n",
    "    embedding = np.zeros((300,))\n",
    "    count = 0\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            embedding += embeddings[word]\n",
    "        except:\n",
    "            count += 1\n",
    "    embedding /= (len(wordlist)-count)\n",
    "    return embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "file = \"../Data/sampledata.txt\"\n",
    "vec = getEmbedding(filename=file,embeddings=embeddings)\n",
    "most_similar = embeddings.most_similar(positive=[vec],topn=3)\n",
    "print(most_similar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ":\n",
      "[('KIM_CLIJSTERS_Yeah', 0.6612500548362732), ('%_#F########_9v.jsn', 0.6536810994148254), ('pioneer_LaLanne', 0.6517382860183716)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}